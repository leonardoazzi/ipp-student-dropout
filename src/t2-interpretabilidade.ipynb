{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpretabilidade e explicabilidade\n",
    "\n",
    "Os alunos deverão explorar algum método de interpretação de modelos e/ou modelos naturalmente interpretáveis (por exemplo, árvores de decisão), a fim de compreender ou extrair hipóteses sobre quais atributos são aparentemente mais relevantes para a tarefa de predição e/ou como eles impactam na decisão do modelo. Sugere-se que se faça esta investigação apenas para um modelo, a ser escolhido com base no melhor desempenho preditivo em dados de teste (isto é, o melhor modelo conforme análise do grupo). Os grupos deverão incluir no relatório informações obtidas desta análise, como gráficos, tabelas, etc, e discutir a respeito das relações encontradas na análise que mais chamaram a atenção, seja pela pertinência da associação ou por ser um resultado inesperado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas plotly matplotlib seaborn scikit-learn shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Módulo para leitura e manipulação dos dados\n",
    "import pandas as pd\n",
    "\n",
    "# Módulo para manipulação de arrays e matrizes\n",
    "import numpy as np\n",
    "\n",
    "# Módulos para visualização de dados e plotagem de gráficos\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Módulos específicos da sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# Módulo para balanceamento de classes\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "import shap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://machinelearningmastery.com/calculate-feature-importance-with-python/\n",
    "\n",
    "You need to be using this version of scikit-learn or higher.\n",
    "0.22.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check scikit-learn version\n",
    "import sklearn\n",
    "print(sklearn.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carregamento dos dados e configuração do modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset pré-processado no notebook [t1-spot-checking.ipynb](./t1-spot-checking.ipynb)\n",
    "\n",
    "---\n",
    "Dataset obtido em https://www.kaggle.com/datasets/thedevastator/higher-education-predictors-of-student-retention/data\n",
    "\n",
    "Original: https://zenodo.org/records/5777340#.Y7FJotJBwUE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./data/clean-dataset.csv\")\n",
    "X = data.drop('Target', axis=1)\n",
    "y = data['Target']\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algoritmos selecionados para otimização de hiperparâmetros\n",
    "\n",
    "lr = LogisticRegression()\n",
    "abc = AdaBoostClassifier()\n",
    "svmachine = svm.SVC(probability=True)\n",
    "\n",
    "algo_dict = {'Logistic Regression': lr, 'AdaBoost': abc, 'SVM': svmachine}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treinando o modelo em todo o dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Interpretabilidade intrínseca\n",
    "\n",
    "**Interpretabilidade:** a capacidade de explicar ou apresentar em termos compreensíveis para um ser humano."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Interpretabilidade extrínseca\n",
    "\n",
    "**Explicabilidade:** coleção de artefatos visuais e/ou interativos que fornecem ao usuário uma descrição suﬁciente do comportamento de\n",
    "um modelo para executar com precisão tarefas como avaliação, conﬁança, previsão ou melhoria de um modelo.\n",
    "\n",
    "**Explainable Machine learning:** análises post-hoc e técnicas usadas para entender os mecanismos para tomada de decisão ou as previsões realizadas pelo modelo treinado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pipeline(model):\n",
    "    steps = list()\n",
    "\n",
    "    steps.append(('Feature Selection', SelectKBest(k=4, score_func=mutual_info_classif)))\n",
    "    steps.append(('Normalização', StandardScaler()))\n",
    "    steps.append(('Balanceamento da classe minoritária', SMOTE(sampling_strategy='minority')))\n",
    "    steps.append(('Modelo', model))\n",
    "\n",
    "    # Cria a pipeline\n",
    "    pipe = Pipeline(steps=steps)\n",
    "\n",
    "    return pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carrega os hiperparâmetros selecionados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "optimal_hyperparams = pd.read_csv(\"./data/otimizacao-hiperparams.csv\")\n",
    "result = optimal_hyperparams.iloc[2]\n",
    "best_params = result.iloc[0]\n",
    "display(optimal_hyperparams)\n",
    "\n",
    "# Convert the string representation of the dictionary to an actual dictionary\n",
    "best_params_dict = ast.literal_eval(best_params)\n",
    "print(best_params_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Treina para todos os dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_pipeline(model=algo_dict['AdaBoost'])\n",
    "model.set_params(Modelo__n_estimators = best_params_dict['Modelo__n_estimators'],\n",
    "                         Modelo__learning_rate = best_params_dict['Modelo__learning_rate'])\n",
    "\n",
    "model.fit(X, y)\n",
    "\n",
    "selected_features = model.named_steps['Feature Selection'].get_support(indices=True)\n",
    "print(\"Selected features:\", selected_features)\n",
    "selected_feature_names = [data.columns[i] for i in selected_features]\n",
    "print(\"Selected feature names:\", selected_feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importância de Atributos\n",
    "\n",
    "- Método de importância de atributos com **testes de permutação é model agnostic**\n",
    "  - https://machinelearningmastery.com/calculate-feature-importance-with-python/\n",
    "  - https://scikit-learn.org/stable/modules/permutation_importance.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Your results may vary given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. **Consider running the example a few times and compare the average outcome.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questões\n",
    "- Devemos usar pipeline?\n",
    "  - https://stackoverflow.com/questions/62106204/permutation-importance-using-a-pipeline-in-scikit-learn\n",
    "- Como importar os hiperparâmetros otimizados?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_indices = {column: index for index, column in enumerate(data.columns)}\n",
    "print(column_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform permutation importance\n",
    "results = permutation_importance(model, X, y, scoring='f1')\n",
    "importance = results.importances_mean\n",
    "\n",
    "# summarize feature importance\n",
    "for i,v in enumerate(importance):\n",
    "\tprint('Feature: %0d, Score: %.5f' % (i,v))\n",
    "\n",
    "# plot feature importance\n",
    "ax = plt.barh([x for x in range(len(importance))], importance)\n",
    "\n",
    "# Show the major grid and style it slightly.\n",
    "plt.grid(which='major', color='#DDDDDD', linewidth=0.8)\n",
    "# Show the minor grid as well. Style it in very light gray as a thin,\n",
    "# dotted line.\n",
    "plt.grid(which='minor', color='#EEEEEE', linestyle=':', linewidth=0.5)\n",
    "\n",
    "# Fixa o eixo x do gráfico\n",
    "plt.xlim(-0.05, max(importance) + 0.1)\n",
    "plt.yticks(ticks=range(len(importance)), labels=[column for column in list(column_indices.keys())[:len(importance)]], rotation=0)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SHAP\n",
    "\n",
    "https://github.com/shap/shap\n",
    "\n",
    "\n",
    "Usando o KernelExplainer, por ser mais agnóstico a modelos, pois o pacote não suporta o AdaBoost diretamente. https://shap.readthedocs.io/en/latest/example_notebooks/tabular_examples/model_agnostic/Simple%20Kernel%20SHAP.html#Using-KernelExplainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the data using the pipeline's feature selection and scaling steps\n",
    "X_transformed = pd.DataFrame(\n",
    "    model.named_steps['Normalização'].transform(model.named_steps['Feature Selection'].transform(X)),\n",
    "    columns=selected_feature_names\n",
    ")\n",
    "\n",
    "# # Pega 100 amostras\n",
    "# X_transformed = shap.sample(X_transformed, 100)\n",
    "\n",
    "# Use SHAP to explain the model's predictions\n",
    "explainer = shap.KernelExplainer(model.named_steps['Modelo'].predict, X_transformed)\n",
    "shap_values = explainer.shap_values(X_transformed)\n",
    "\n",
    "# Plot the SHAP values\n",
    "shap.summary_plot(shap_values, X_transformed)\n",
    "\n",
    "shap.plots.bar(shap_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, _ in algo_dict.items():\n",
    "    metrics_per_model = metrics_df[metrics_df['Algoritmo'] == name]\n",
    "\n",
    "    mean = float(round(metrics_per_model['f1'].mean(),3))\n",
    "    std = float(round(metrics_per_model['f1'].std(),3))\n",
    "    print(f'F1-Score do {name}: ', mean, f'({std})')\n",
    "\n",
    "    mean = float(round(metrics_per_model['precision'].mean(),3))\n",
    "    std = float(round(metrics_per_model['precision'].std(),3))\n",
    "    print(f'Precisão média do {name}: ', mean, f'({std})')\n",
    "\n",
    "    mean = float(round(metrics_per_model['recall'].mean(),3))\n",
    "    std = float(round(metrics_per_model['recall'].std(),3))\n",
    "    print(f'Recall média do {name}: ', mean, f'({std})')\n",
    "\n",
    "    mean = float(round(metrics_per_model['roc_auc'].mean(),3))\n",
    "    std = float(round(metrics_per_model['roc_auc'].std(),3))\n",
    "    print(f'ROC AUC média do {name}: ', mean, f'({std})')\n",
    "\n",
    "    print('\\n')\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demora bastante para usar o KernelExplainer do SHAP\n",
    "\n",
    "https://github.com/ModelOriented/DALEX/issues/366"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PDP\n",
    "https://scikit-learn.org/stable/modules/partial_dependence.html\n",
    "\n",
    "IDP: unlike a PDP, which shows the average effect of the input feature, an ICE plot visualizes the dependence of the prediction on a feature for each sample separately with one line per sample. Due to the limits of human perception, only one input feature of interest is supported for ICE plots. While the PDPs are good at showing the average effect of the target features, they can obscure a heterogeneous relationship created by interactions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import PartialDependenceDisplay\n",
    "\n",
    "# One-way Partial Dependence Plot\n",
    "# ==================================\n",
    "features = list(range(20)) # Índices dos atributos que serão plotados\n",
    "\n",
    "PartialDependenceDisplay.from_estimator(model, X, features, n_cols=3)\n",
    "plt.gcf().set_size_inches(15, 30)\n",
    "\n",
    "PartialDependenceDisplay.from_estimator(model, X, features, n_cols=3, kind='both')\n",
    "plt.gcf().set_size_inches(15, 30)\n",
    "\n",
    "# Two-way Partial Dependence Plot\n",
    "# ==================================\n",
    "\n",
    "# Grade 2nd sem vs. Scholarship holder\n",
    "features = [(13,15)]\n",
    "PartialDependenceDisplay.from_estimator(model, X, features)\n",
    "\n",
    "# Tuition Fees Up to Date vs. Grade 2nd sem\n",
    "features = [(11,15)]\n",
    "PartialDependenceDisplay.from_estimator(model, X, features)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
