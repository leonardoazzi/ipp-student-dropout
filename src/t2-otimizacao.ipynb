{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Otimização de hiperparâmetros e avaliação final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas plotly matplotlib seaborn scikit-learn xgboost optuna hyperopt setuptools nbformat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Módulo para leitura e manipulação dos dados\n",
    "import pandas as pd\n",
    "\n",
    "# Módulo para manipulação de arrays e matrizes\n",
    "import numpy as np\n",
    "\n",
    "# Módulos para visualização de dados e plotagem de gráficos\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Módulos específicos da sklearn\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.metrics import f1_score,precision_score,recall_score, roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "\n",
    "# Módulo para balanceamento de classes\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "# Otimizador\n",
    "import optuna\n",
    "from optuna.visualization import plot_optimization_history, plot_slice, plot_contour, plot_parallel_coordinate, plot_param_importances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carregamento dos dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset pré-processado no notebook [t1-spot-checking.ipynb](./t1-spot-checking.ipynb)\n",
    "\n",
    "---\n",
    "Dataset obtido em https://www.kaggle.com/datasets/thedevastator/higher-education-predictors-of-student-retention/data\n",
    "\n",
    "Original: https://zenodo.org/records/5777340#.Y7FJotJBwUE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./data/clean-dataset.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Otimização de hiperparâmetros\n",
    "\n",
    "Cada algoritmo escolhido como resultado do T1 deve ser treinado para obter o melhor desempenho possível. Não havendo um referencial de desempenho para o problema, os grupos devem reportar o melhor modelo dentre todos os que foram treinados e avaliados. Isso envolve o melhor ajuste de hiperparâmetros que for possível encontrar. Automatizar essa etapa de otimização de hiperparâmetros pode economizar bastante esforço. O scikit-learn oferece o GridSearch que é uma busca exaustiva trivial. Assim, bibliotecas que implementam algoritmos mais “inteligentes”, como Hyperopt e Optuna, podem ser um diferencial na obtenção de um bom desempenho.\n",
    "\n",
    "Os grupos devem garantir que a metodologia de pré-processamento, treino e validação esteja correta. Um aspecto particularmente crítico é a prevenção de data leakage (ie, vazamento de dados), que pode comprometer os resultados e a validade do modelo. Por exemplo, o cálculo das estatísticas de pré-processamento (e.g. fit do scikit-learn para normalização, balanceamento de dados e imputação) só pode ser feito nos dados de treino. Esse cálculo é, então, aplicado (e.g. transform do scikit-learn) nos dados de treino, validação e teste. Além disso, a otimização de hiperparâmetros deve ser avaliada no conjunto de validação e não no de teste.\n",
    "\n",
    "A divisão dos dados em treino, validação e teste pode ser feita via holdout de 3-vias ou holdout+validação cruzada (CV), onde o holdout separa o de teste e o restante é dividido conforme a validação cruzada, ou validação cruzada (CV) aninhada. Os grupos devem avaliar qual a melhor estratégia, balanceando a confiabilidade do método (e.g. validação cruzada aninhada garante mais variabilidade nos dados) vs o tempo de execução (e.g. 5-folds externos e 3-folds internos de CV aninhada fazem com que cada possível modelo seja treinado e avaliado 15 vezes). Independente do método escolhido, a divisão dos dados deve ser estratificada (e.g. mesmo percentual de cada classe no treino, validação e teste)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algoritmos selecionados para otimização de hiperparâmetros\n",
    "\n",
    "lr = LogisticRegression()\n",
    "abc = AdaBoostClassifier()\n",
    "svmachine = svm.SVC(probability=True)\n",
    "\n",
    "algo_dict = {'Logistic Regression': lr, 'AdaBoost': abc, 'SVM': svmachine}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Referências\n",
    "# https://machinelearningmastery.com/spot-check-machine-learning-algorithms-in-python/\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html\n",
    "def make_pipeline(model):\n",
    "    steps = list()\n",
    "\n",
    "    steps.append(('Feature Selection', SelectKBest(k=4, score_func=mutual_info_classif)))\n",
    "    steps.append(('Normalização', StandardScaler()))\n",
    "    steps.append(('Balanceamento da classe minoritária', SMOTE(sampling_strategy='minority')))\n",
    "    steps.append(('Modelo', model))\n",
    "\n",
    "    # Cria a pipeline\n",
    "    pipe = Pipeline(steps=steps)\n",
    "\n",
    "    return pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_optuna_space(model_name, trial):\n",
    "    \n",
    "    # Define o espaço de busca de hiperparâmetros para cada tipo de modelo\n",
    "    if model_name == 'Logistic Regression':\n",
    "\n",
    "        # Senão, com mais hiperparâmetros:\n",
    "        solver_penalty_str = trial.suggest_categorical(\n",
    "            'solver_penalty',\n",
    "            ['liblinear_l1', 'liblinear_l2', 'lbfgs_l2']\n",
    "        )\n",
    "\n",
    "        # Quebra as strings de maneira a ter o solver e a penalidade certa\n",
    "        solver, penalty = solver_penalty_str.split('_')\n",
    "\n",
    "        return {\n",
    "            'Modelo__C': trial.suggest_float('C', 1e-3, 1e3, log=True),\n",
    "            'Modelo__solver': solver,\n",
    "            'Modelo__penalty': penalty\n",
    "        }\n",
    "\n",
    "    elif model_name == 'AdaBoost':\n",
    "        # Para AdaBoost, ajusta o número de estimadores e a taxa de aprendizado\n",
    "        return {\n",
    "            'Modelo__n_estimators': trial.suggest_int('n_estimators', 50, 500),\n",
    "            'Modelo__learning_rate': trial.suggest_float('learning_rate', 0.01, 2, log=True)\n",
    "        }\n",
    "    elif model_name == 'SVM':\n",
    "        # Para SVM, ajusta o parâmetro C e o tipo de kernel\n",
    "        return {\n",
    "            'Modelo__C': trial.suggest_float('C', 1e-4, 1e2, log=True),\n",
    "            'Modelo__kernel': trial.suggest_categorical('kernel', ['linear', 'rbf', 'sigmoid'])\n",
    "        }\n",
    "    else:\n",
    "        # Se nenhum hiperparâmetro estiver definido no modelo, retorna um dicionário vazio\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_function(trial, model_name, model, X_train, y_train, inner_folds, metric):\n",
    "    # Define o espaço de busca dos hiperparâmetros\n",
    "    params = define_optuna_space(model_name, trial)\n",
    "\n",
    "    # Cria o pipeline com seleção de atributos, normalização, balanceamento e o modelo\n",
    "    pipeline = make_pipeline(model)\n",
    "\n",
    "    # Ajusta o pipeline com os hiperparâmetros sugeridos pelo trial do optuna\n",
    "    pipeline.set_params(**params)\n",
    "\n",
    "    # Define o k-fold interno para validação cruzada\n",
    "    inner_cv = StratifiedKFold(n_splits=inner_folds, shuffle=True, random_state=42)\n",
    "\n",
    "    # Calcula a métrica de avaliação média (por ex. F1) através de validação cruzada\n",
    "    score = cross_val_score(pipeline, X_train, y_train, cv=inner_cv, scoring=metric, n_jobs=-1).mean()\n",
    "    return score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Executa o estudo do Optuna para um único fold externo e retorna o melhor estudo\n",
    "def run_optuna_study_for_fold(model_name, model, X_train, y_train, inner_folds, metric, n_trials=20):\n",
    "\n",
    "    # Função objetivo que o Optuna irá otimizar\n",
    "    def objective(trial):\n",
    "        return objective_function(trial, model_name, model, X_train, y_train, inner_folds, metric)\n",
    "\n",
    "    # Cria o estudo do Optuna com objetivo de maximizar a métrica\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "\n",
    "    # Executa o estudo com número definido de tentativas (n_trials)\n",
    "    study.optimize(objective, n_trials=n_trials)\n",
    "    return study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avalia o modelo com os melhores parâmetros encontrados no conjunto de teste.\n",
    "def evaluate_best_params(model, model_name, best_params, fold_number, X_train, y_train, X_test, y_test):\n",
    "    pipeline = make_pipeline(model)\n",
    "    pipeline.set_params(**best_params) # Ajusta o pipeline com os melhores parâmetros\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "\n",
    "    return {\n",
    "        'Algoritmo': model_name,\n",
    "        'Número do Outer Fold': fold_number,\n",
    "        'Parâmetros': best_params,\n",
    "        'F1 Score': f1_score(y_test, y_pred),\n",
    "        'Precisão': precision_score(y_test, y_pred),\n",
    "        'Revocação': recall_score(y_test, y_pred),\n",
    "        'ROC AUC': roc_auc_score(y_test, pipeline.predict_proba(X_test)[:, 1])\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separa atributos preditivos e atributo alvo\n",
    "X = data.drop('Target', axis=1)\n",
    "y = data['Target']\n",
    "\n",
    "# Dividindo os dados entre treino e teste. Os dados de treino serão utilizados no Nested CV, \n",
    "# e os dados de teste serão utilizados posteriormente para avaliar os modelos\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.3, \n",
    "                                                    random_state=42, \n",
    "                                                    stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nested CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nested_cv_with_optuna(X, y, models, outer_folds=5, inner_folds=3, metric='f1', n_trials=20):\n",
    "\n",
    "    # Define o k-fold externo para a nested cv\n",
    "    outer_cv = StratifiedKFold(n_splits=outer_folds, shuffle=True, random_state=42)\n",
    "    results = []\n",
    "    best_study_per_model = {}\n",
    "    best_params_per_model = {}\n",
    "    best_f1_per_model = {}\n",
    "\n",
    "    # Loop pelos modelos que serão ajustados\n",
    "    for model_name, model in models.items():\n",
    "        best_f1 = -1\n",
    "        best_study = None\n",
    "\n",
    "        # Loop do k-fold externo\n",
    "        for fold_number, (train_idx, test_idx) in enumerate(outer_cv.split(X, y)):\n",
    "            X_outer_train, X_outer_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "            y_outer_train, y_outer_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "            # Executa o estudo do Optuna para o fold atual\n",
    "            study = run_optuna_study_for_fold(model_name, model, X_outer_train, y_outer_train, inner_folds, metric, n_trials=n_trials)\n",
    "\n",
    "            # Obtém os melhores parâmetros do estudo\n",
    "            best_params = {f'Modelo__{key}': value for key, value in study.best_params.items()}\n",
    "\n",
    "            # Retira os pares de strings solver_penalty, caso presentes\n",
    "            if 'Modelo__solver_penalty' in best_params:\n",
    "                del best_params['Modelo__solver_penalty']\n",
    "\n",
    "            # Avalia no conjunto de teste com os melhores parâmetros\n",
    "            metrics = evaluate_best_params(model, model_name, best_params, fold_number, X_outer_train, y_outer_train, X_outer_test, y_outer_test)\n",
    "            metrics['Algoritmo'] = model_name\n",
    "            results.append(metrics)\n",
    "\n",
    "            # Verifica se a pontuação (F1) melhorou em relação ao melhor caso anterior\n",
    "            if metrics['F1 Score'] > best_f1:\n",
    "                best_f1 = metrics['F1 Score']\n",
    "                best_study = study\n",
    "                best_params_per_model[model_name] = best_params\n",
    "\n",
    "        best_study_per_model[model_name] = best_study\n",
    "        best_f1_per_model[model_name] = best_f1\n",
    "\n",
    "    # Retorna os resultados consolidados, o melhor estudo, os melhores parâmetros e a melhor pontuação F1 por modelo\n",
    "    return pd.DataFrame(results), best_study_per_model, best_params_per_model, best_f1_per_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Roda o Nested CV para otimização de hiperparâmetros entre Regressão Logística, AdaBoost e SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna_algo_dict = {\n",
    "    'Logistic Regression': algo_dict['Logistic Regression'],\n",
    "    'AdaBoost': algo_dict['AdaBoost'],\n",
    "    'SVM': algo_dict['SVM']\n",
    "}\n",
    "\n",
    "output = nested_cv_with_optuna(X_train, y_train, \n",
    "                                    optuna_algo_dict, \n",
    "                                    outer_folds=10,\n",
    "                                    n_trials=50, \n",
    "                                    inner_folds=5, \n",
    "                                    metric='f1')\n",
    "\n",
    "results_df, best_study_per_model, best_params_per_model, best_f1_per_model = output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizações da otimização de hiperparâmetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name in best_params_per_model:\n",
    "    print(f\"Melhores parâmetros para {model_name}: {best_params_per_model[model_name]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exibe os melhores resultados e parâmetros para cada modelo\n",
    "for model_name in best_study_per_model:\n",
    "    print(f\"Best F1 for {model_name}: {best_f1_per_model[model_name]}\")\n",
    "    print(f\"Best Parameters for {model_name}: {best_params_per_model[model_name]}\")\n",
    "\n",
    "    print(f\"Plots for model: {model_name}\")\n",
    "\n",
    "    study = best_study_per_model[model_name]\n",
    "    fig_opt_history = plot_optimization_history(study)\n",
    "    fig_opt_history.show()\n",
    "\n",
    "    fig_slice = plot_slice(study)\n",
    "    fig_slice.show()\n",
    "\n",
    "    fig_parallel = plot_parallel_coordinate(study)\n",
    "    fig_parallel.show()\n",
    "\n",
    "    fig_contour = plot_contour(study)\n",
    "    fig_contour.show()\n",
    "\n",
    "    fig_param_importances = plot_param_importances(study)\n",
    "    fig_param_importances.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Métricas da otimização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def letter_annotation(ax, xoffset, yoffset, letter):\n",
    " ax.text(xoffset, yoffset, letter, transform=ax.transAxes,\n",
    "         size=12)\n",
    "\n",
    "fig = plt.figure(figsize=(12, 14))\n",
    "\n",
    "# Cria 2 subfiguras para a primeira e segunda linha\n",
    "(row1fig, row2fig) = fig.subfigures(2, 1, height_ratios=[1,1])\n",
    "\n",
    "# Primeira linha\n",
    "row1_axs = row1fig.subplots(1, 2)\n",
    "row1fig.subplots_adjust(wspace=0.3, hspace=0.01, left=0, right=1.2, bottom=.2)\n",
    "\n",
    "# Segunda linha\n",
    "row2_axs = row2fig.subplots(1, 2)\n",
    "row2fig.subplots_adjust(wspace=0.3, hspace=0.01, left=0, right=1.2, bottom=.3)\n",
    "\n",
    "# F1-Score\n",
    "# ============================================================\n",
    "ax = row1_axs[0]\n",
    "sns.boxplot(data=results_df, x='Algoritmo', y='F1 Score', hue='Algoritmo', palette='Set3', ax=ax)\n",
    "ax.tick_params(labelrotation=45)\n",
    "ax.set_ylim(0.5, 1.0)\n",
    "ax.set_title('F1-Score por algoritmo otimizado')\n",
    "letter_annotation(ax, -.15, 1, 'a)')\n",
    "sns.despine(offset=5, trim=False, ax=ax)\n",
    "\n",
    "# Precisão\n",
    "# ============================================================\n",
    "ax = row1_axs[1]\n",
    "sns.boxplot(data=results_df, x='Algoritmo', y='Precisão', hue='Algoritmo', palette='Set3', ax=ax)\n",
    "ax.tick_params(labelrotation=45)\n",
    "ax.set_ylim(0.5, 1.0)\n",
    "ax.set_title('Precisão por algoritmo otimizado')\n",
    "letter_annotation(ax, -.15, 1, 'b)')\n",
    "sns.despine(offset=5, trim=False, ax=ax)\n",
    "\n",
    "# Recall\n",
    "# ============================================================\n",
    "ax = row2_axs[0]\n",
    "sns.boxplot(data=results_df, x='Algoritmo', y='Revocação', hue='Algoritmo', palette='Set3', ax=ax)\n",
    "ax.tick_params(labelrotation=45)\n",
    "ax.set_ylim(0.5, 1.0)\n",
    "ax.set_title('Revocação por algoritmo otimizado')\n",
    "letter_annotation(ax, -.15, 1, 'c)')\n",
    "sns.despine(offset=5, trim=False, ax=ax)\n",
    "\n",
    "# ROC AUC\n",
    "# ============================================================\n",
    "ax = row2_axs[1]\n",
    "sns.boxplot(data=results_df, x='Algoritmo', y='ROC AUC', hue='Algoritmo', palette='Set3', ax=ax)\n",
    "ax.tick_params(labelrotation=45)\n",
    "ax.set_ylim(0.5, 1.0)\n",
    "ax.set_title('Área sob a curva ROC por algoritmo otimizado')\n",
    "letter_annotation(ax, -.15, 1, 'd)')\n",
    "sns.despine(offset=5, trim=False, ax=ax)\n",
    "\n",
    "# ============================================================\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seleção de modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mostra os resultados do Outer Fold, sumarizando as avaliações no fold de teste e melhores parâmetros estudados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecionaremos para o treinamento final os algoritmos com melhor F1-Score, nossa métrica principal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_f1_score_and_params = results_df.loc[results_df.groupby('Algoritmo')['F1 Score'].idxmax(), ['Algoritmo', 'F1 Score', 'Parâmetros']].set_index('Algoritmo').to_dict('index')\n",
    "\n",
    "print(\"Melhores F1 Score e respectivos parâmetros de cada algoritmo:\")\n",
    "print(best_f1_score_and_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Avaliação de desempenho final\n",
    "\n",
    "Utilizem as métricas definidas na primeira parte do trabalho para comparar os modelos, garantindo que a métrica seja a correta para o problema, considerando eventuais desbalanceamentos nos dados.\n",
    "Após a otimização de hiperparâmetros de todos os modelos candidatos (isto é, aqueles treinados com 2-3 estratégias mais promissoras escolhidas com base em T1), o desempenho dos modelos nos dados de teste deve ser comparado para identificar qual modelo oferece o melhor desempenho global e quais são os pontos fortes e fracos de cada modelo. Com base nesta análise, os grupos devem tentar definir o modelo mais indicado para atacar o problema abordado.\n",
    "\n",
    "Criem gráficos e visualizações que ajudem a ilustrar as comparações entre os modelos. Conforme apropriado, obtenham curvas ROC ou PR, gráficos de barras comparando as métricas de cada modelo, e/ou matrizes de confusão.\n",
    "\n",
    "Identifiquem o melhor modelo, ou se não há um modelo claramente melhor, quais são os “vencedores” e as situações e características de vantagem e desvantagem entre eles. Verifiquem os casos (instâncias/amostras) onde o melhor modelo (ou os melhores, em caso de “empate”) vai(vão) bem ou mal. A análise não deve ser pra cada amostra individual, mas de maneira agregada. Por exemplo, se o problema for de classificação, analise qual é a classe mais difícil para o modelo, e se há amostras “confusas” no sentido que seus atributos parecem realmente pertencer a outra classe. Se o problema é de regressão, veja se há algum padrão onde os erros são maiores.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Após otimizar e encontrar a melhor pipeline de modelagem, realizaremos seu treinamento em todo o conjunto treino, para finalmente validar no conjunto de teste, reservado anteriormente para este momento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = best_f1_score_and_params['Logistic Regression']['Parâmetros']\n",
    "print(best_params)\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "best_pipeline = make_pipeline(model=algo_dict['Logistic Regression'])\n",
    "best_pipeline.set_params(Modelo__C = best_params['Modelo__C'])\n",
    "\n",
    "\n",
    "best_pipeline.fit(X_train, y_train)\n",
    "y_pred = best_pipeline.predict(X_test)\n",
    "y_proba = best_pipeline.predict_proba(X_test)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_results_lr = {\n",
    "    'F1 Score': f1_score(y_test, y_pred),\n",
    "    'Precisão': precision_score(y_test, y_pred),\n",
    "    'Revocação': recall_score(y_test, y_pred),\n",
    "    'ROC AUC': roc_auc_score(y_test, y_proba)\n",
    "}\n",
    "\n",
    "print(\"Avaliação final no conjunto de testes\", final_results_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert final_results into a DataFrame for easy plotting\n",
    "final_results_df = pd.DataFrame(list(final_results_lr.items()), columns=['Metric', 'Value'])\n",
    "\n",
    "# Set a style for seaborn\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Create a figure and axes\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Plot the bar chart\n",
    "sns.barplot(x='Metric', y='Value', data=final_results_df, palette='Blues_d')\n",
    "\n",
    "# Add labels and title\n",
    "plt.title(\"Avaliação final no conjunto de teste\", fontsize=16)\n",
    "plt.ylim(0, 1.0)\n",
    "for index, row in final_results_df.iterrows():\n",
    "    plt.text(index, row['Value'] + 0.01, f\"{row['Value']:.3f}\", \n",
    "             ha='center', va='bottom', fontsize=11)\n",
    "\n",
    "plt.xlabel(\"\")\n",
    "plt.ylabel(\"Pontuação\")\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_proba)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, label=f\"ROC AUC = {final_results_lr['ROC AUC']:.3f}\")\n",
    "plt.plot([0, 1], [0, 1], 'r--', label=\"Random Guessing\")\n",
    "\n",
    "plt.title(\"Curva ROC para Regressão Logística no conjunto de teste\", fontsize=16)\n",
    "plt.xlabel(\"Taxa de Falsos Positivos\", fontsize=14)\n",
    "plt.ylabel(\"Taxa de Verdadeiros Positivos (Revocação)\", fontsize=14)\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Plot using the built-in confusion matrix display\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Negativo', 'Positivo'])\n",
    "disp.plot(cmap='Blues', values_format='d')\n",
    "plt.title(\"Matriz de confusão para Regressão Logística no conjunto de teste\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = best_f1_score_and_params['AdaBoost']['Parâmetros']\n",
    "print(best_params)\n",
    "\n",
    "best_pipeline = make_pipeline(model=algo_dict['AdaBoost'])\n",
    "best_pipeline.set_params(Modelo__n_estimators=best_params['Modelo__n_estimators'],\n",
    "                         Modelo__learning_rate=best_params['Modelo__learning_rate'])\n",
    "\n",
    "best_pipeline.fit(X_train, y_train)\n",
    "y_pred = best_pipeline.predict(X_test)\n",
    "y_proba = best_pipeline.predict_proba(X_test)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_results_abc = {\n",
    "    'F1 Score': f1_score(y_test, y_pred),\n",
    "    'Precisão': precision_score(y_test, y_pred),\n",
    "    'Revocação': recall_score(y_test, y_pred),\n",
    "    'ROC AUC': roc_auc_score(y_test, y_proba)\n",
    "}\n",
    "\n",
    "print(\"Avaliação final no conjunto de testes\", final_results_abc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert final_results into a DataFrame for easy plotting\n",
    "final_results_df = pd.DataFrame(list(final_results_abc.items()), columns=['Metric', 'Value'])\n",
    "\n",
    "# Set a style for seaborn\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Create a figure and axes\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Plot the bar chart\n",
    "sns.barplot(x='Metric', y='Value', data=final_results_df, palette='Blues_d')\n",
    "\n",
    "# Add labels and title\n",
    "plt.title(\"Avaliação final no conjunto de teste\", fontsize=16)\n",
    "plt.ylim(0, 1.0)\n",
    "for index, row in final_results_df.iterrows():\n",
    "    plt.text(index, row['Value'] + 0.01, f\"{row['Value']:.3f}\", \n",
    "             ha='center', va='bottom', fontsize=11)\n",
    "\n",
    "plt.xlabel(\"\")\n",
    "plt.ylabel(\"Pontuação\")\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_proba)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, label=f\"ROC AUC = {final_results_abc['ROC AUC']:.3f}\")\n",
    "plt.plot([0, 1], [0, 1], 'r--', label=\"Random Guessing\")\n",
    "\n",
    "plt.title(\"Curva ROC para AdaBoost no conjunto de teste\", fontsize=16)\n",
    "plt.xlabel(\"Taxa de Falsos Positivos\", fontsize=14)\n",
    "plt.ylabel(\"Taxa de Verdadeiros Positivos (Revocação)\", fontsize=14)\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Plot using the built-in confusion matrix display\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Negative', 'Positive'])\n",
    "disp.plot(cmap='Blues', values_format='d')\n",
    "plt.title(\"Matriz de confusão para Regressão Logística no conjunto de teste\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = best_f1_score_and_params['SVM']['Parâmetros']\n",
    "print(best_params)\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "best_pipeline = make_pipeline(model=algo_dict['SVM'])\n",
    "best_pipeline.set_params(Modelo__C = best_params['Modelo__C'], \n",
    "                         Modelo__kernel = best_params['Modelo__kernel'])\n",
    "\n",
    "best_pipeline.fit(X_train, y_train)\n",
    "y_pred = best_pipeline.predict(X_test)\n",
    "y_proba = best_pipeline.predict_proba(X_test)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_results_svm = {\n",
    "    'F1 Score': f1_score(y_test, y_pred),\n",
    "    'Precisão': precision_score(y_test, y_pred),\n",
    "    'Revocação': recall_score(y_test, y_pred),\n",
    "    'ROC AUC': roc_auc_score(y_test, y_proba)\n",
    "}\n",
    "\n",
    "print(\"Avaliação final no conjunto de testes\", final_results_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert final_results into a DataFrame for easy plotting\n",
    "final_results_df = pd.DataFrame(list(final_results_svm.items()), columns=['Metric', 'Value'])\n",
    "\n",
    "# Set a style for seaborn\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Create a figure and axes\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Plot the bar chart\n",
    "sns.barplot(x='Metric', y='Value', data=final_results_df, palette='Blues_d')\n",
    "\n",
    "# Add labels and title\n",
    "plt.title(\"Avaliação final no conjunto de teste\", fontsize=16)\n",
    "plt.ylim(0, 1.0)\n",
    "for index, row in final_results_df.iterrows():\n",
    "    plt.text(index, row['Value'] + 0.01, f\"{row['Value']:.3f}\", \n",
    "             ha='center', va='bottom', fontsize=11)\n",
    "\n",
    "plt.xlabel(\"\")\n",
    "plt.ylabel(\"Pontuação\")\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_proba)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, label=f\"ROC AUC = {final_results_svm['ROC AUC']:.3f}\")\n",
    "plt.plot([0, 1], [0, 1], 'r--', label=\"Random Guessing\")\n",
    "\n",
    "plt.title(\"Curva ROC para SVM no conjunto de teste\", fontsize=16)\n",
    "plt.xlabel(\"Taxa de Falsos Positivos\", fontsize=14)\n",
    "plt.ylabel(\"Taxa de Verdadeiros Positivos (Revocação)\", fontsize=14)\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Plot using the built-in confusion matrix display\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Negative', 'Positive'])\n",
    "disp.plot(cmap='Blues', values_format='d')\n",
    "plt.title(\"Matriz de confusão para Regressão Logística no conjunto de teste\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Comparação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def letter_annotation(ax, xoffset, yoffset, letter):\n",
    " ax.text(xoffset, yoffset, letter, transform=ax.transAxes,\n",
    "         size=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 14))\n",
    "\n",
    "# Cria 2 subfiguras para a primeira e segunda linha\n",
    "(row1fig, row2fig) = fig.subfigures(2, 1, height_ratios=[1,1])\n",
    "\n",
    "# Primeira linha\n",
    "row1_axs = row1fig.subplots(1, 2)\n",
    "row1fig.subplots_adjust(wspace=0.3, hspace=0.01, left=0, right=1.2, bottom=.2)\n",
    "\n",
    "# Segunda linha\n",
    "row2_axs = row2fig.subplots(1, 2)\n",
    "row2fig.subplots_adjust(wspace=0.3, hspace=0.01, left=0, right=1.2, bottom=.3)\n",
    "\n",
    "\n",
    "# Dados dos F1 Scores dos modelos\n",
    "final_results = {\n",
    "    'Logistic Regression': final_results_lr['F1 Score'],\n",
    "    'AdaBoost': final_results_abc['F1 Score'],\n",
    "    'SVM': final_results_svm['F1 Score']\n",
    "}\n",
    "\n",
    "# Dados das precisões dos modelos\n",
    "precision_results = {\n",
    "    'Logistic Regression': final_results_lr['Precisão'],\n",
    "    'AdaBoost': final_results_abc['Precisão'],\n",
    "    'SVM': final_results_svm['Precisão']\n",
    "}\n",
    "\n",
    "# Convertendo os dados para um DataFrame\n",
    "precision_results_df = pd.DataFrame(list(precision_results.items()), columns=['Algoritmo', 'Precisão'])\n",
    "display(precision_results_df)\n",
    "\n",
    "# Dados das revocações dos modelos\n",
    "recall_results = {\n",
    "    'Logistic Regression': final_results_lr['Revocação'],\n",
    "    'AdaBoost': final_results_abc['Revocação'],\n",
    "    'SVM': final_results_svm['Revocação']\n",
    "}\n",
    "\n",
    "# Convertendo os dados para um DataFrame\n",
    "recall_results_df = pd.DataFrame(list(recall_results.items()), columns=['Algoritmo', 'Revocação'])\n",
    "\n",
    "# Dados das ROC AUC dos modelos\n",
    "roc_auc_results = {\n",
    "    'Logistic Regression': final_results_lr['ROC AUC'],\n",
    "    'AdaBoost': final_results_abc['ROC AUC'],\n",
    "    'SVM': final_results_svm['ROC AUC']\n",
    "}\n",
    "\n",
    "# Convertendo os dados para um DataFrame\n",
    "roc_auc_results_df = pd.DataFrame(list(roc_auc_results.items()), columns=['Algoritmo', 'ROC AUC'])\n",
    "\n",
    "# Convertendo os dados para um DataFrame\n",
    "final_df = pd.DataFrame(list(final_results.items()), columns=['Algoritmo', 'F1 Score'])\n",
    "\n",
    "# == F1 Score ================================================\n",
    "ax = row1_axs[0]\n",
    "sns.barplot(x='Algoritmo', y='F1 Score', data=final_df, hue='Algoritmo', palette='viridis', ax=ax)\n",
    "ax.set_title('Comparação de F1 Score entre os modelos no conjunto de teste', fontsize=12)\n",
    "for index, row in final_df.iterrows():\n",
    "    ax.text(index, row['F1 Score'] + 0.01, f\"{row['F1 Score']:.3f}\",\n",
    "             ha='center', va='bottom', fontsize=11)\n",
    "ax.set_xlabel('Modelos', fontsize=12)\n",
    "ax.set_ylabel('F1 Score', fontsize=12)\n",
    "ax.tick_params(labelrotation=45)\n",
    "ax.set_ylim(0.5, 1)\n",
    "letter_annotation(ax, -.15, 1, 'a)')\n",
    "\n",
    "# == Precisão ================================================\n",
    "ax = row1_axs[1]\n",
    "sns.barplot(x='Algoritmo', y='Precisão', data=precision_results_df, hue='Algoritmo', palette='viridis', ax=ax)\n",
    "ax.set_title('Comparação da Precisão entre os modelos no conjunto de teste', fontsize=12)\n",
    "for index, row in precision_results_df.iterrows():\n",
    "    ax.text(index, row['Precisão'] + 0.01, f\"{row['Precisão']:.3f}\",\n",
    "             ha='center', va='bottom', fontsize=11)\n",
    "ax.set_xlabel('Modelos', fontsize=12)\n",
    "ax.set_ylabel('Precisão', fontsize=12)\n",
    "ax.tick_params(labelrotation=45)\n",
    "ax.set_ylim(0.5, 1)\n",
    "letter_annotation(ax, -.15, 1, 'b)')\n",
    "\n",
    "# == Revocaçao ================================================\n",
    "ax = row2_axs[0]\n",
    "sns.barplot(x='Algoritmo', y='Revocação', data=recall_results_df, hue='Algoritmo', palette='viridis', ax=ax)\n",
    "ax.set_title('Comparação da Revocação entre os modelos no conjunto de teste', fontsize=12)\n",
    "for index, row in recall_results_df.iterrows():\n",
    "    ax.text(index, row['Revocação'] + 0.01, f\"{row['Revocação']:.3f}\",\n",
    "             ha='center', va='bottom', fontsize=11)\n",
    "ax.set_xlabel('Modelos', fontsize=12)\n",
    "ax.set_ylabel('Revocação', fontsize=12)\n",
    "ax.tick_params(labelrotation=45)\n",
    "ax.set_ylim(0.5, 1)\n",
    "letter_annotation(ax, -.15, 1, 'c)')\n",
    "\n",
    "# == ROC AUC ================================================\n",
    "ax = row2_axs[1]\n",
    "sns.barplot(x='Algoritmo', y='ROC AUC', data=roc_auc_results_df, hue='Algoritmo', palette='viridis')\n",
    "ax.set_title('Comparação de ROC AUC entre os modelos no conjunto de teste', fontsize=12)\n",
    "for index, row in roc_auc_results_df.iterrows():\n",
    "    ax.text(index, row['ROC AUC'] + 0.01, f\"{row['ROC AUC']:.3f}\",\n",
    "             ha='center', va='bottom', fontsize=11)\n",
    "ax.set_xlabel('Modelos', fontsize=12)\n",
    "ax.set_ylabel('ROC AUC', fontsize=12)\n",
    "ax.tick_params(labelrotation=45)\n",
    "ax.set_ylim(0.5, 1)\n",
    "letter_annotation(ax, -.15, 1, 'd)')\n",
    "\n",
    "# ============================================================\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dados das precisões dos modelos\n",
    "precision_results = {\n",
    "    'Logistic Regression': final_results_lr['Precisão'],\n",
    "    'AdaBoost': final_results_abc['Precisão'],\n",
    "    'SVM': final_results_svm['Precisão']\n",
    "}\n",
    "\n",
    "# Convertendo os dados para um DataFrame\n",
    "precision_results_df = pd.DataFrame(list(precision_results.items()), columns=['Algoritmo', 'Precisão'])\n",
    "display(precision_results_df)\n",
    "\n",
    "# Plotando o gráfico de barras\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Algoritmo', y='Precisão', data=precision_results_df, palette='viridis')\n",
    "\n",
    "# Adicionando rótulos e título\n",
    "plt.title('Comparação da Precisão entre os modelos no conjunto de teste', fontsize=16)\n",
    "plt.ylim(0, 1.0)\n",
    "for index, row in precision_results_df.iterrows():\n",
    "    plt.text(index, row['Precisão'] + 0.01, f\"{row['Precisão']:.3f}\", \n",
    "             ha='center', va='bottom', fontsize=11)\n",
    "\n",
    "plt.xlabel('Modelos', fontsize=14)\n",
    "plt.ylabel('Precisão', fontsize=14)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "# Dados das revocações dos modelos\n",
    "recall_results = {\n",
    "    'Logistic Regression': final_results_lr['Revocação'],\n",
    "    'AdaBoost': final_results_abc['Revocação'],\n",
    "    'SVM': final_results_svm['Revocação']\n",
    "}\n",
    "\n",
    "# Convertendo os dados para um DataFrame\n",
    "recall_results_df = pd.DataFrame(list(recall_results.items()), columns=['Algoritmo', 'Revocação'])\n",
    "\n",
    "# Plotando o gráfico de barras\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Algoritmo', y='Revocação', data=recall_results_df, palette='viridis')\n",
    "\n",
    "# Adicionando rótulos e título\n",
    "plt.title('Comparação da Revocação entre os modelos no conjunto de teste', fontsize=16)\n",
    "plt.ylim(0, 1.0)\n",
    "for index, row in recall_results_df.iterrows():\n",
    "    plt.text(index, row['Revocação'] + 0.01, f\"{row['Revocação']:.3f}\", \n",
    "             ha='center', va='bottom', fontsize=11)\n",
    "\n",
    "plt.xlabel('Modelos', fontsize=14)\n",
    "plt.ylabel('Revocação', fontsize=14)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dados das ROC AUC dos modelos\n",
    "roc_auc_results = {\n",
    "    'Logistic Regression': final_results_lr['ROC AUC'],\n",
    "    'AdaBoost': final_results_abc['ROC AUC'],\n",
    "    'SVM': final_results_svm['ROC AUC']\n",
    "}\n",
    "\n",
    "# Convertendo os dados para um DataFrame\n",
    "roc_auc_results_df = pd.DataFrame(list(roc_auc_results.items()), columns=['Algoritmo', 'ROC AUC'])\n",
    "\n",
    "# Plotando o gráfico de barras\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Algoritmo', y='ROC AUC', data=roc_auc_results_df, palette='viridis')\n",
    "\n",
    "# Adicionando rótulos e título\n",
    "plt.title('Comparação de ROC AUC entre os modelos no conjunto de teste', fontsize=16)\n",
    "plt.ylim(0, 1.0)\n",
    "for index, row in roc_auc_results_df.iterrows():\n",
    "    plt.text(index, row['ROC AUC'] + 0.01, f\"{row['ROC AUC']:.3f}\", \n",
    "             ha='center', va='bottom', fontsize=11)\n",
    "\n",
    "plt.xlabel('Modelos', fontsize=14)\n",
    "plt.ylabel('ROC AUC', fontsize=14)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exporta hiperparâmetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforma dicionário em Data Frame\n",
    "best_f1_score_and_params_df = pd.DataFrame(best_f1_score_and_params).T.reset_index()\n",
    "best_f1_score_and_params_df.columns = ['Algoritmo', 'F1 Score', 'Parâmetros']\n",
    "\n",
    "# Salva o Data Frame em um arquivo CSV\n",
    "best_f1_score_and_params_df.to_csv('./data/otimizacao-hiperparams.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
