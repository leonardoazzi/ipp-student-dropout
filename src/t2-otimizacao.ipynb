{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introdução\n",
    "O notebook abaixo apresenta e implementa o spot-checking de modelos preditivos supervisionados, desenvolvido para primeiro trabalho da disciplina Aprendizado de Máquina da Universidade Federal do Rio Grande do Sul (2024/2).\n",
    "\n",
    "Neste trabalho, buscamos analisar a relação de diversos fatores, como gênero e notas do primeiro semestre, com a taxa de desistência de alunos. No modelo abaixo usamos o dataset carregado nesse notebook, analisamos quais os fatores que de fato influenciam na desistência dos alunos e possibilitamos que inputs personalizados sejam adicionados ao modelo para que seja calculado a probabilidade de um aluno desistir do curso.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas plotly matplotlib seaborn scikit-learn xgboost optuna hyperopt setuptools nbformat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Módulo para leitura e manipulação dos dados\n",
    "import pandas as pd\n",
    "\n",
    "# Módulo para manipulação de arrays e matrizes\n",
    "import numpy as np\n",
    "\n",
    "# Módulos para visualização de dados e plotagem de gráficos\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Módulos específicos da sklearn\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.metrics import accuracy_score,f1_score,precision_score,recall_score, roc_auc_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "# Biblioteca com algoritmos específicos de machine learning\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Módulo para balanceamento de classes\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "import optuna\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Carregamento dos dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset pré-processado no notebook [t1-spot-checking.ipynb](./t1-spot-checking.ipynb)\n",
    "\n",
    "---\n",
    "Dataset obtido em https://www.kaggle.com/datasets/thedevastator/higher-education-predictors-of-student-retention/data\n",
    "\n",
    "Original: https://zenodo.org/records/5777340#.Y7FJotJBwUE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../data/clean-dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separa atributos preditivos e atributo alvo\n",
    "X = data.drop('Target', axis=1)\n",
    "y = data['Target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algoritmos selecionados para treinamento\n",
    "dtree = DecisionTreeClassifier(random_state=0)\n",
    "dtree2 = DecisionTreeClassifier(random_state=0, max_depth=10)\n",
    "rfc_gini = RandomForestClassifier(random_state=2)\n",
    "rfc_entropy = RandomForestClassifier(random_state=2, criterion='entropy')\n",
    "lr = LogisticRegression(random_state=42)\n",
    "knn_3 = KNeighborsClassifier(n_neighbors=3)\n",
    "knn_5 = KNeighborsClassifier(n_neighbors=5)\n",
    "abc = AdaBoostClassifier(n_estimators=50,learning_rate=1, random_state=0, algorithm='SAMME')\n",
    "svmachine = svm.SVC(kernel='linear',probability=True)\n",
    "\n",
    "algo_dict = {'Decision Tree': dtree, 'Decision Tree Max depth 5': dtree2, 'Random Forest gini': rfc_gini, 'Random Forest entropy': rfc_entropy, 'Logistic Regression': lr, '3-Nearest Neighbors': knn_3, '5-Nearest Neighbors': knn_5, 'AdaBoost': abc, 'SVM': svmachine}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Referências\n",
    "# https://machinelearningmastery.com/spot-check-machine-learning-algorithms-in-python/\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html\n",
    "def make_pipeline(model):\n",
    "    steps = list()\n",
    "\n",
    "    steps.append(('Feature Selection', SelectKBest(k=10)))\n",
    "    steps.append(('Normalização', StandardScaler()))\n",
    "    steps.append(('Balanceamento da classe minoritária', SMOTE(sampling_strategy='minority')))\n",
    "    steps.append(('Modelo', model))\n",
    "\n",
    "    # Cria a pipeline\n",
    "    pipe = Pipeline(steps=steps)\n",
    "\n",
    "    return pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def define_grid_search_params(model_name):\n",
    "#     if model_name == 'Random Forest gini' or model_name == 'Random Forest entropy':\n",
    "#         return {\n",
    "#             'Modelo__n_estimators': [50, 100, 150],\n",
    "#             'Modelo__max_depth': [10, 20, 30]\n",
    "#         }\n",
    "#     elif model_name == 'Decision Tree' or model_name == 'Decision Tree Max depth 5':\n",
    "#         return {\n",
    "#             'Modelo__max_depth': [5, 10, 20, 30]\n",
    "#         }\n",
    "#     elif model_name == 'Logistic Regression':\n",
    "#         return {\n",
    "#             'Modelo__C': [0.01, 0.1, 1, 10, 100]\n",
    "#         }\n",
    "#     elif model_name == '3-Nearest Neighbors' or model_name == '5-Nearest Neighbors':\n",
    "#         return {\n",
    "#             'Modelo__n_neighbors': [3, 5, 7, 10]\n",
    "#         }\n",
    "#     elif model_name == 'AdaBoost':\n",
    "#         return {\n",
    "#             'Modelo__n_estimators': [50, 100, 150],\n",
    "#             'Modelo__learning_rate': [0.01, 0.1, 1]\n",
    "#         }\n",
    "#     elif model_name == 'SVM':\n",
    "#         return {\n",
    "#             'Modelo__C': [0.01, 0.1, 1, 10],\n",
    "#             'Modelo__kernel': ['linear', 'rbf']\n",
    "#         }\n",
    "#     else:\n",
    "#         raise ValueError(f\"Não foi definido parâmetro para o modelo: {model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def nested_cv_with_gridsearch(X, y, models, outer_folds=5, inner_folds=3, metric='f1'):\n",
    "#     outer_cv = StratifiedKFold(n_splits=outer_folds, shuffle=True, random_state=42)\n",
    "#     results = []\n",
    "\n",
    "#     for model_name, model in models.items():\n",
    "#         param_grid = define_grid_search_params(model_name)\n",
    "\n",
    "#         for train_idx, test_idx in outer_cv.split(X, y):\n",
    "#             X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "#             y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "#             pipeline = make_pipeline(model)\n",
    "\n",
    "#             grid_search = GridSearchCV(\n",
    "#                 estimator=pipeline,\n",
    "#                 param_grid=param_grid,\n",
    "#                 cv=StratifiedKFold(n_splits=inner_folds, shuffle=True, random_state=42),\n",
    "#                 scoring=metric,\n",
    "#                 n_jobs=-1\n",
    "#             )\n",
    "\n",
    "#             grid_search.fit(X_train, y_train)\n",
    "\n",
    "#             best_pipeline = grid_search.best_estimator_\n",
    "#             y_pred = best_pipeline.predict(X_test)\n",
    "\n",
    "#             metrics = {\n",
    "#                 'Model': model_name,\n",
    "#                 'F1 Score': f1_score(y_test, y_pred),\n",
    "#                 'Precision': precision_score(y_test, y_pred),\n",
    "#                 'Recall': recall_score(y_test, y_pred),\n",
    "#                 'ROC AUC': roc_auc_score(y_test, best_pipeline.predict_proba(X_test)[:, 1])\n",
    "#             }\n",
    "#             results.append(metrics)\n",
    "\n",
    "#     return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_df = nested_cv_with_gridsearch(X, y, algo_dict, outer_folds=5, inner_folds=3, metric='f1')\n",
    "# display(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def define_hyperopt_space(model_name):\n",
    "#     if model_name == 'Random Forest gini' or model_name == 'Random Forest entropy':\n",
    "#         return {\n",
    "#             'n_estimators': hp.quniform('n_estimators', 50, 300, 10),\n",
    "#             'max_depth': hp.quniform('max_depth', 5, 50, 1),\n",
    "#         }\n",
    "#     elif model_name == 'Decision Tree' or model_name == 'Decision Tree Max depth 5':\n",
    "#         return {\n",
    "#             'max_depth': hp.quniform('max_depth', 5, 50, 1),\n",
    "#         }\n",
    "#     elif model_name == 'Logistic Regression':\n",
    "#         return {\n",
    "#             'C': hp.loguniform('C', np.log(1e-4), np.log(1e2)),\n",
    "#         }\n",
    "#     elif model_name == '3-Nearest Neighbors' or model_name == '5-Nearest Neighbors':\n",
    "#         return {\n",
    "#             'n_neighbors': hp.quniform('n_neighbors', 3, 20, 1),\n",
    "#         }\n",
    "#     elif model_name == 'AdaBoost':\n",
    "#         return {\n",
    "#             'n_estimators': hp.quniform('n_estimators', 50, 300, 10),\n",
    "#             'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(2)),\n",
    "#         }\n",
    "#     elif model_name == 'SVM':\n",
    "#         return {\n",
    "#             'C': hp.loguniform('C', np.log(1e-4), np.log(1e2)),\n",
    "#             'kernel': hp.choice('kernel', ['linear', 'rbf']),\n",
    "#         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def nested_cv_with_hyperopt(X, y, models, outer_folds=5, inner_folds=3, metric='f1'):\n",
    "#     outer_cv = StratifiedKFold(n_splits=outer_folds, shuffle=True, random_state=42)\n",
    "#     results = []\n",
    "\n",
    "#     for model_name, model in models.items():\n",
    "\n",
    "#         # Define objective function for Hyperopt\n",
    "#         def objective(params):\n",
    "#             # If SVM, map the kernel index to string for correct cross-validation\n",
    "#             if 'kernel' in params and model_name == 'SVM':\n",
    "#                 kernel_mapping = ['linear', 'rbf']\n",
    "#                 if isinstance(params['kernel'], int):\n",
    "#                     params['kernel'] = kernel_mapping[params['kernel']]\n",
    "\n",
    "#             pipeline = make_pipeline(model)\n",
    "\n",
    "#             # Convert integer-like floats to int\n",
    "#             final_params = {}\n",
    "#             for k, v in params.items():\n",
    "#                 if isinstance(v, float) and v.is_integer():\n",
    "#                     v = int(v)  # Convert to int if it is a whole number float\n",
    "#                 final_params[f'Modelo__{k}'] = v\n",
    "\n",
    "#             pipeline.set_params(**final_params)\n",
    "\n",
    "#             inner_cv = StratifiedKFold(n_splits=inner_folds, shuffle=True, random_state=42)\n",
    "#             scores = cross_val_score(pipeline, X_train, y_train, cv=inner_cv, scoring=metric, n_jobs=-1)\n",
    "#             return {'loss': -np.mean(scores), 'status': STATUS_OK}\n",
    "\n",
    "#         # Outer loop\n",
    "#         for train_idx, test_idx in outer_cv.split(X, y):\n",
    "#             X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "#             y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "#             search_space = define_hyperopt_space(model_name)\n",
    "#             trials = Trials()\n",
    "\n",
    "#             best_params = fmin(\n",
    "#                 fn=objective,\n",
    "#                 space=search_space,\n",
    "#                 algo=tpe.suggest,\n",
    "#                 max_evals=20,\n",
    "#                 trials=trials,\n",
    "#             )\n",
    "\n",
    "#             # Map SVM kernel back if needed\n",
    "#             if model_name == 'SVM' and 'kernel' in best_params:\n",
    "#                 kernel_mapping = ['linear', 'rbf']\n",
    "#                 best_params['kernel'] = kernel_mapping[best_params['kernel']]\n",
    "\n",
    "#             # Convert parameters and retrain pipeline\n",
    "#             final_params = {}\n",
    "#             for k, v in best_params.items():\n",
    "#                 if isinstance(v, float) and v.is_integer():\n",
    "#                     v = int(v)\n",
    "#                 final_params[f'Modelo__{k}'] = v\n",
    "\n",
    "#             # Train the pipeline with the best parameters\n",
    "#             pipeline = make_pipeline(model)\n",
    "#             pipeline.set_params(**final_params)\n",
    "#             pipeline.fit(X_train, y_train)\n",
    "\n",
    "#             # Evaluate on the test set\n",
    "#             y_pred = pipeline.predict(X_test)\n",
    "#             metrics = {\n",
    "#                 'Model': model_name,\n",
    "#                 'F1 Score': f1_score(y_test, y_pred),\n",
    "#                 'Precision': precision_score(y_test, y_pred),\n",
    "#                 'Recall': recall_score(y_test, y_pred),\n",
    "#                 'ROC AUC': roc_auc_score(y_test, pipeline.predict_proba(X_test)[:, 1])\n",
    "#             }\n",
    "#             results.append(metrics)\n",
    "\n",
    "#     return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_df = nested_cv_with_hyperopt(X, y, algo_dict, outer_folds=5, inner_folds=3, metric='f1')\n",
    "# display(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_optuna_space(model_name, trial):\n",
    "    if model_name == 'Random Forest gini' or model_name == 'Random Forest entropy':\n",
    "        return {\n",
    "            'Modelo__n_estimators': trial.suggest_int('n_estimators', 50, 300),\n",
    "            'Modelo__max_depth': trial.suggest_int('max_depth', 5, 50)\n",
    "        }\n",
    "    elif model_name == 'Decision Tree' or model_name == 'Decision Tree Max depth 5':\n",
    "        return {\n",
    "            'Modelo__max_depth': trial.suggest_int('max_depth', 5, 50)\n",
    "        }\n",
    "    elif model_name == 'Logistic Regression':\n",
    "        return {\n",
    "            'Modelo__C': trial.suggest_loguniform('C', 1e-4, 1e2)\n",
    "        }\n",
    "    elif model_name == '3-Nearest Neighbors' or model_name == '5-Nearest Neighbors':\n",
    "        return {\n",
    "            'Modelo__n_neighbors': trial.suggest_int('n_neighbors', 3, 20)\n",
    "        }\n",
    "    elif model_name == 'AdaBoost':\n",
    "        return {\n",
    "            'Modelo__n_estimators': trial.suggest_int('n_estimators', 50, 300),\n",
    "            'Modelo__learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 2)\n",
    "        }\n",
    "    elif model_name == 'SVM':\n",
    "        return {\n",
    "            'Modelo__C': trial.suggest_loguniform('C', 1e-4, 1e2),\n",
    "            'Modelo__kernel': trial.suggest_categorical('kernel', ['linear', 'rbf'])\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from optuna.visualization import (\n",
    "    plot_optimization_history, \n",
    "    plot_slice, \n",
    "    plot_contour, \n",
    "    plot_parallel_coordinate, \n",
    "    plot_param_importances\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nested_cv_with_optuna(X, y, models, outer_folds=5, inner_folds=4, metric='f1'):\n",
    "    outer_cv = StratifiedKFold(n_splits=outer_folds, shuffle=True, random_state=42)\n",
    "    results = []\n",
    "    studies = {}  # Dictionary to store the last study for each model\n",
    "\n",
    "    for model_name, model in models.items():\n",
    "        def objective(trial):\n",
    "            params = define_optuna_space(model_name, trial)\n",
    "\n",
    "            pipeline = make_pipeline(model)\n",
    "            pipeline.set_params(**params)\n",
    "            inner_cv = StratifiedKFold(n_splits=inner_folds, shuffle=True, random_state=42)\n",
    "\n",
    "            return cross_val_score(pipeline, X_train, y_train, cv=inner_cv, scoring=metric, n_jobs=-1).mean()\n",
    "\n",
    "        for train_idx, test_idx in outer_cv.split(X, y):\n",
    "            X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "            y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "            study = optuna.create_study(direction=\"maximize\")\n",
    "            study.optimize(objective, n_trials=20)\n",
    "            studies[model_name] = study  # Store the last study for this model\n",
    "\n",
    "            best_params = {f'Modelo__{key}': value for key, value in study.best_params.items()}\n",
    "\n",
    "            pipeline = make_pipeline(model)\n",
    "            pipeline.set_params(**best_params)\n",
    "            pipeline.fit(X_train, y_train)\n",
    "\n",
    "            y_pred = pipeline.predict(X_test)\n",
    "            metrics = {\n",
    "                'Model': model_name,\n",
    "                'F1 Score': f1_score(y_test, y_pred),\n",
    "                'Precision': precision_score(y_test, y_pred),\n",
    "                'Recall': recall_score(y_test, y_pred),\n",
    "                'ROC AUC': roc_auc_score(y_test, pipeline.predict_proba(X_test)[:, 1])\n",
    "            }\n",
    "            results.append(metrics)\n",
    "\n",
    "    return pd.DataFrame(results), studies  # Return results and all studies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna_algo_dict = {\n",
    "    'Logistic Regression': algo_dict['Logistic Regression'],\n",
    "    'AdaBoost': algo_dict['AdaBoost'],\n",
    "    'SVM': algo_dict['SVM']\n",
    "}\n",
    "\n",
    "results_df, studies = nested_cv_with_optuna(X, y, optuna_algo_dict, outer_folds=5, inner_folds=3, metric='f1')\n",
    "display(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate plots for each model\n",
    "for model_name, study in studies.items():\n",
    "    print(f\"Plots for model: {model_name}\")\n",
    "    \n",
    "    # Optimization history\n",
    "    fig_opt_history = plot_optimization_history(study)\n",
    "    fig_opt_history.show()\n",
    "\n",
    "    # Slice plot\n",
    "    fig_slice = plot_slice(study)\n",
    "    fig_slice.show()\n",
    "\n",
    "    # Parallel coordinate plot\n",
    "    fig_parallel = plot_parallel_coordinate(study)\n",
    "    fig_parallel.show()\n",
    "\n",
    "    # Contour plot\n",
    "    fig_contour = plot_contour(study)\n",
    "    fig_contour.show()\n",
    "\n",
    "    # Parameter importances\n",
    "    fig_param_importances = plot_param_importances(study)\n",
    "    fig_param_importances.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
