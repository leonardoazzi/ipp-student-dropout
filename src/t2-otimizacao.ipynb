{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introdução\n",
    "O notebook abaixo apresenta e implementa o spot-checking de modelos preditivos supervisionados, desenvolvido para primeiro trabalho da disciplina Aprendizado de Máquina da Universidade Federal do Rio Grande do Sul (2024/2).\n",
    "\n",
    "Neste trabalho, buscamos analisar a relação de diversos fatores, como gênero e notas do primeiro semestre, com a taxa de desistência de alunos. No modelo abaixo usamos o dataset carregado nesse notebook, analisamos quais os fatores que de fato influenciam na desistência dos alunos e possibilitamos que inputs personalizados sejam adicionados ao modelo para que seja calculado a probabilidade de um aluno desistir do curso.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas plotly matplotlib seaborn scikit-learn xgboost optuna hyperopt setuptools nbformat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Módulo para leitura e manipulação dos dados\n",
    "import pandas as pd\n",
    "\n",
    "# Módulo para manipulação de arrays e matrizes\n",
    "import numpy as np\n",
    "\n",
    "# Módulos para visualização de dados e plotagem de gráficos\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Módulos específicos da sklearn\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.metrics import accuracy_score,f1_score,precision_score,recall_score, roc_auc_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "\n",
    "# Biblioteca com algoritmos específicos de machine learning\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Módulo para balanceamento de classes\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "import optuna\n",
    "from optuna.visualization import plot_optimization_history, plot_slice, plot_contour, plot_parallel_coordinate, plot_param_importances\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Carregamento dos dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset pré-processado no notebook [t1-spot-checking.ipynb](./t1-spot-checking.ipynb)\n",
    "\n",
    "---\n",
    "Dataset obtido em https://www.kaggle.com/datasets/thedevastator/higher-education-predictors-of-student-retention/data\n",
    "\n",
    "Original: https://zenodo.org/records/5777340#.Y7FJotJBwUE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../data/clean-dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separa atributos preditivos e atributo alvo\n",
    "X = data.drop('Target', axis=1)\n",
    "y = data['Target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algoritmos selecionados para treinamento\n",
    "dtree = DecisionTreeClassifier(random_state=0)\n",
    "dtree2 = DecisionTreeClassifier(random_state=0, max_depth=10)\n",
    "rfc_gini = RandomForestClassifier(random_state=2)\n",
    "rfc_entropy = RandomForestClassifier(random_state=2, criterion='entropy')\n",
    "lr = LogisticRegression(random_state=42)\n",
    "knn_3 = KNeighborsClassifier(n_neighbors=3)\n",
    "knn_5 = KNeighborsClassifier(n_neighbors=5)\n",
    "abc = AdaBoostClassifier(n_estimators=50,learning_rate=1, random_state=0, algorithm='SAMME')\n",
    "svmachine = svm.SVC(kernel='linear',probability=True)\n",
    "\n",
    "algo_dict = {'Decision Tree': dtree, 'Decision Tree Max depth 5': dtree2, 'Random Forest gini': rfc_gini, 'Random Forest entropy': rfc_entropy, 'Logistic Regression': lr, '3-Nearest Neighbors': knn_3, '5-Nearest Neighbors': knn_5, 'AdaBoost': abc, 'SVM': svmachine}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Referências\n",
    "# https://machinelearningmastery.com/spot-check-machine-learning-algorithms-in-python/\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html\n",
    "def make_pipeline(model):\n",
    "    steps = list()\n",
    "\n",
    "    steps.append(('Feature Selection', SelectKBest(k=10, score_func=mutual_info_classif)))\n",
    "    steps.append(('Normalização', StandardScaler()))\n",
    "    steps.append(('Balanceamento da classe minoritária', SMOTE(sampling_strategy='minority')))\n",
    "    steps.append(('Modelo', model))\n",
    "\n",
    "    # Cria a pipeline\n",
    "    pipe = Pipeline(steps=steps)\n",
    "\n",
    "    return pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def define_grid_search_params(model_name):\n",
    "#     if model_name == 'Random Forest gini' or model_name == 'Random Forest entropy':\n",
    "#         return {\n",
    "#             'Modelo__n_estimators': [50, 100, 150],\n",
    "#             'Modelo__max_depth': [10, 20, 30]\n",
    "#         }\n",
    "#     elif model_name == 'Decision Tree' or model_name == 'Decision Tree Max depth 5':\n",
    "#         return {\n",
    "#             'Modelo__max_depth': [5, 10, 20, 30]\n",
    "#         }\n",
    "#     elif model_name == 'Logistic Regression':\n",
    "#         return {\n",
    "#             'Modelo__C': [0.01, 0.1, 1, 10, 100]\n",
    "#         }\n",
    "#     elif model_name == '3-Nearest Neighbors' or model_name == '5-Nearest Neighbors':\n",
    "#         return {\n",
    "#             'Modelo__n_neighbors': [3, 5, 7, 10]\n",
    "#         }\n",
    "#     elif model_name == 'AdaBoost':\n",
    "#         return {\n",
    "#             'Modelo__n_estimators': [50, 100, 150],\n",
    "#             'Modelo__learning_rate': [0.01, 0.1, 1]\n",
    "#         }\n",
    "#     elif model_name == 'SVM':\n",
    "#         return {\n",
    "#             'Modelo__C': [0.01, 0.1, 1, 10],\n",
    "#             'Modelo__kernel': ['linear', 'rbf']\n",
    "#         }\n",
    "#     else:\n",
    "#         raise ValueError(f\"Não foi definido parâmetro para o modelo: {model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def nested_cv_with_gridsearch(X, y, models, outer_folds=5, inner_folds=3, metric='f1'):\n",
    "#     outer_cv = StratifiedKFold(n_splits=outer_folds, shuffle=True, random_state=42)\n",
    "#     results = []\n",
    "\n",
    "#     for model_name, model in models.items():\n",
    "#         param_grid = define_grid_search_params(model_name)\n",
    "\n",
    "#         for train_idx, test_idx in outer_cv.split(X, y):\n",
    "#             X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "#             y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "#             pipeline = make_pipeline(model)\n",
    "\n",
    "#             grid_search = GridSearchCV(\n",
    "#                 estimator=pipeline,\n",
    "#                 param_grid=param_grid,\n",
    "#                 cv=StratifiedKFold(n_splits=inner_folds, shuffle=True, random_state=42),\n",
    "#                 scoring=metric,\n",
    "#                 n_jobs=-1\n",
    "#             )\n",
    "\n",
    "#             grid_search.fit(X_train, y_train)\n",
    "\n",
    "#             best_pipeline = grid_search.best_estimator_\n",
    "#             y_pred = best_pipeline.predict(X_test)\n",
    "\n",
    "#             metrics = {\n",
    "#                 'Model': model_name,\n",
    "#                 'F1 Score': f1_score(y_test, y_pred),\n",
    "#                 'Precision': precision_score(y_test, y_pred),\n",
    "#                 'Recall': recall_score(y_test, y_pred),\n",
    "#                 'ROC AUC': roc_auc_score(y_test, best_pipeline.predict_proba(X_test)[:, 1])\n",
    "#             }\n",
    "#             results.append(metrics)\n",
    "\n",
    "#     return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_df = nested_cv_with_gridsearch(X, y, algo_dict, outer_folds=5, inner_folds=3, metric='f1')\n",
    "# display(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def define_hyperopt_space(model_name):\n",
    "#     if model_name == 'Random Forest gini' or model_name == 'Random Forest entropy':\n",
    "#         return {\n",
    "#             'n_estimators': hp.quniform('n_estimators', 50, 300, 10),\n",
    "#             'max_depth': hp.quniform('max_depth', 5, 50, 1),\n",
    "#         }\n",
    "#     elif model_name == 'Decision Tree' or model_name == 'Decision Tree Max depth 5':\n",
    "#         return {\n",
    "#             'max_depth': hp.quniform('max_depth', 5, 50, 1),\n",
    "#         }\n",
    "#     elif model_name == 'Logistic Regression':\n",
    "#         return {\n",
    "#             'C': hp.loguniform('C', np.log(1e-4), np.log(1e2)),\n",
    "#         }\n",
    "#     elif model_name == '3-Nearest Neighbors' or model_name == '5-Nearest Neighbors':\n",
    "#         return {\n",
    "#             'n_neighbors': hp.quniform('n_neighbors', 3, 20, 1),\n",
    "#         }\n",
    "#     elif model_name == 'AdaBoost':\n",
    "#         return {\n",
    "#             'n_estimators': hp.quniform('n_estimators', 50, 300, 10),\n",
    "#             'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(2)),\n",
    "#         }\n",
    "#     elif model_name == 'SVM':\n",
    "#         return {\n",
    "#             'C': hp.loguniform('C', np.log(1e-4), np.log(1e2)),\n",
    "#             'kernel': hp.choice('kernel', ['linear', 'rbf']),\n",
    "#         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def nested_cv_with_hyperopt(X, y, models, outer_folds=5, inner_folds=3, metric='f1'):\n",
    "#     outer_cv = StratifiedKFold(n_splits=outer_folds, shuffle=True, random_state=42)\n",
    "#     results = []\n",
    "\n",
    "#     for model_name, model in models.items():\n",
    "\n",
    "#         # Define objective function for Hyperopt\n",
    "#         def objective(params):\n",
    "#             # If SVM, map the kernel index to string for correct cross-validation\n",
    "#             if 'kernel' in params and model_name == 'SVM':\n",
    "#                 kernel_mapping = ['linear', 'rbf']\n",
    "#                 if isinstance(params['kernel'], int):\n",
    "#                     params['kernel'] = kernel_mapping[params['kernel']]\n",
    "\n",
    "#             pipeline = make_pipeline(model)\n",
    "\n",
    "#             # Convert integer-like floats to int\n",
    "#             final_params = {}\n",
    "#             for k, v in params.items():\n",
    "#                 if isinstance(v, float) and v.is_integer():\n",
    "#                     v = int(v)  # Convert to int if it is a whole number float\n",
    "#                 final_params[f'Modelo__{k}'] = v\n",
    "\n",
    "#             pipeline.set_params(**final_params)\n",
    "\n",
    "#             inner_cv = StratifiedKFold(n_splits=inner_folds, shuffle=True, random_state=42)\n",
    "#             scores = cross_val_score(pipeline, X_train, y_train, cv=inner_cv, scoring=metric, n_jobs=-1)\n",
    "#             return {'loss': -np.mean(scores), 'status': STATUS_OK}\n",
    "\n",
    "#         # Outer loop\n",
    "#         for train_idx, test_idx in outer_cv.split(X, y):\n",
    "#             X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "#             y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "#             search_space = define_hyperopt_space(model_name)\n",
    "#             trials = Trials()\n",
    "\n",
    "#             best_params = fmin(\n",
    "#                 fn=objective,\n",
    "#                 space=search_space,\n",
    "#                 algo=tpe.suggest,\n",
    "#                 max_evals=20,\n",
    "#                 trials=trials,\n",
    "#             )\n",
    "\n",
    "#             # Map SVM kernel back if needed\n",
    "#             if model_name == 'SVM' and 'kernel' in best_params:\n",
    "#                 kernel_mapping = ['linear', 'rbf']\n",
    "#                 best_params['kernel'] = kernel_mapping[best_params['kernel']]\n",
    "\n",
    "#             # Convert parameters and retrain pipeline\n",
    "#             final_params = {}\n",
    "#             for k, v in best_params.items():\n",
    "#                 if isinstance(v, float) and v.is_integer():\n",
    "#                     v = int(v)\n",
    "#                 final_params[f'Modelo__{k}'] = v\n",
    "\n",
    "#             # Train the pipeline with the best parameters\n",
    "#             pipeline = make_pipeline(model)\n",
    "#             pipeline.set_params(**final_params)\n",
    "#             pipeline.fit(X_train, y_train)\n",
    "\n",
    "#             # Evaluate on the test set\n",
    "#             y_pred = pipeline.predict(X_test)\n",
    "#             metrics = {\n",
    "#                 'Model': model_name,\n",
    "#                 'F1 Score': f1_score(y_test, y_pred),\n",
    "#                 'Precision': precision_score(y_test, y_pred),\n",
    "#                 'Recall': recall_score(y_test, y_pred),\n",
    "#                 'ROC AUC': roc_auc_score(y_test, pipeline.predict_proba(X_test)[:, 1])\n",
    "#             }\n",
    "#             results.append(metrics)\n",
    "\n",
    "#     return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_df = nested_cv_with_hyperopt(X, y, algo_dict, outer_folds=5, inner_folds=3, metric='f1')\n",
    "# display(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def define_optuna_space(model_name, trial):\n",
    "#     if model_name == 'Random Forest gini' or model_name == 'Random Forest entropy':\n",
    "#         return {\n",
    "#             'Modelo__n_estimators': trial.suggest_int('n_estimators', 50, 300),\n",
    "#             'Modelo__max_depth': trial.suggest_int('max_depth', 5, 50)\n",
    "#         }\n",
    "#     elif model_name == 'Decision Tree' or model_name == 'Decision Tree Max depth 5':\n",
    "#         return {\n",
    "#             'Modelo__max_depth': trial.suggest_int('max_depth', 5, 50)\n",
    "#         }\n",
    "#     elif model_name == 'Logistic Regression':\n",
    "#         return {\n",
    "#             'Modelo__C': trial.suggest_float('C', 1e-3, 1e3, log=True)\n",
    "#         }\n",
    "#     elif model_name == '3-Nearest Neighbors' or model_name == '5-Nearest Neighbors':\n",
    "#         return {\n",
    "#             'Modelo__n_neighbors': trial.suggest_int('n_neighbors', 3, 20)\n",
    "#         }\n",
    "#     elif model_name == 'AdaBoost':\n",
    "#         return {\n",
    "#             'Modelo__n_estimators': trial.suggest_int('n_estimators', 50, 500),\n",
    "#             'Modelo__learning_rate': trial.suggest_float('learning_rate', 0.01, 2, log=True)\n",
    "#         }\n",
    "#     elif model_name == 'SVM':\n",
    "#         return {\n",
    "#             'Modelo__C': trial.suggest_float('C', 1e-4, 1e2, log=True),\n",
    "#             'Modelo__kernel': trial.suggest_categorical('kernel', ['linear', 'rbf'])\n",
    "#         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_optuna_space(model_name, trial):\n",
    "    \n",
    "    # Define o espaço de busca de hiperparâmetros para cada tipo de modelo\n",
    "    if model_name == 'Logistic Regression':\n",
    "\n",
    "        # Caso a gente queira fazer uma avaliação mais simples:\n",
    "        # # Para regressão logística, ajusta o parâmetro de regularização C\n",
    "        # return {\n",
    "        #     'Modelo__C': trial.suggest_float('C', 1e-3, 1e3, log=True)\n",
    "        # }\n",
    "\n",
    "        # Senão, com mais hiperparâmetros:\n",
    "        solver_penalty_str = trial.suggest_categorical(\n",
    "            'solver_penalty',\n",
    "            ['liblinear_l1', 'liblinear_l2', 'lbfgs_l2']\n",
    "        )\n",
    "\n",
    "        # Quebra as strings de maneira a ter o solver e a penalidade certa\n",
    "        solver, penalty = solver_penalty_str.split('_')\n",
    "\n",
    "        return {\n",
    "            'Modelo__C': trial.suggest_float('C', 1e-3, 1e3, log=True),\n",
    "            'Modelo__solver': solver,\n",
    "            'Modelo__penalty': penalty\n",
    "        }\n",
    "\n",
    "    elif model_name == 'AdaBoost':\n",
    "        # Para AdaBoost, ajusta o número de estimadores e a taxa de aprendizado\n",
    "        return {\n",
    "            'Modelo__n_estimators': trial.suggest_int('n_estimators', 50, 500),\n",
    "            'Modelo__learning_rate': trial.suggest_float('learning_rate', 0.01, 2, log=True)\n",
    "        }\n",
    "    elif model_name == 'SVM':\n",
    "        # Para SVM, ajusta o parâmetro C e o tipo de kernel\n",
    "        return {\n",
    "            'Modelo__C': trial.suggest_float('C', 1e-4, 1e2, log=True),\n",
    "            'Modelo__kernel': trial.suggest_categorical('kernel', ['linear', 'rbf', 'sigmoid'])\n",
    "        }\n",
    "    else:\n",
    "        # Se nenhum hiperparâmetro estiver definido no modelo, retorna um dicionário vazio\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_function(trial, model_name, model, X_train, y_train, inner_folds, metric):\n",
    "    # Define o espaço de busca dos hiperparâmetros\n",
    "    params = define_optuna_space(model_name, trial)\n",
    "\n",
    "    # Cria o pipeline com seleção de atributos, normalização, balanceamento e o modelo\n",
    "    pipeline = make_pipeline(model)\n",
    "\n",
    "    # Ajusta o pipeline com os hiperparâmetros sugeridos pelo trial do optuna\n",
    "    pipeline.set_params(**params)\n",
    "\n",
    "    # Define o k-fold interno para validação cruzada\n",
    "    inner_cv = StratifiedKFold(n_splits=inner_folds, shuffle=True, random_state=42)\n",
    "\n",
    "    # Calcula a métrica de avaliação média (por ex. F1) através de validação cruzada\n",
    "    score = cross_val_score(pipeline, X_train, y_train, cv=inner_cv, scoring=metric, n_jobs=-1).mean()\n",
    "    return score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Executa o estudo do Optuna para um único fold externo e retorna o melhor estudo\n",
    "def run_optuna_study_for_fold(model_name, model, X_train, y_train, inner_folds, metric, n_trials=20):\n",
    "\n",
    "    # Função objetivo que o Optuna irá otimizar\n",
    "    def objective(trial):\n",
    "        return objective_function(trial, model_name, model, X_train, y_train, inner_folds, metric)\n",
    "\n",
    "    # Cria o estudo do Optuna com objetivo de maximizar a métrica\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "\n",
    "    # Executa o estudo com número definido de tentativas (n_trials)\n",
    "    study.optimize(objective, n_trials=n_trials)\n",
    "    return study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avalia o modelo com os melhores parâmetros encontrados no conjunto de teste.\n",
    "def evaluate_best_params(model, best_params, X_train, y_train, X_test, y_test):\n",
    "    pipeline = make_pipeline(model)\n",
    "    pipeline.set_params(**best_params) # Ajusta o pipeline com os melhores parâmetros\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "\n",
    "    return {\n",
    "        'F1 Score': f1_score(y_test, y_pred),\n",
    "        'Precision': precision_score(y_test, y_pred),\n",
    "        'Recall': recall_score(y_test, y_pred),\n",
    "        'ROC AUC': roc_auc_score(y_test, pipeline.predict_proba(X_test)[:, 1])\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nested_cv_with_optuna(X, y, models, outer_folds=4, inner_folds=2, metric='f1'):\n",
    "\n",
    "    # Define o k-fold externo para a nested cv\n",
    "    outer_cv = StratifiedKFold(n_splits=outer_folds, shuffle=True, random_state=42)\n",
    "    results = []\n",
    "    best_study_per_model = {}\n",
    "    best_params_per_model = {}\n",
    "    best_f1_per_model = {}\n",
    "    all_fold_params_per_model = {}\n",
    "\n",
    "    # Loop pelos modelos que serão ajustados\n",
    "    for model_name, model in models.items():\n",
    "        # Vamos armazenar o melhor estudo e parâmetros por conveniência, \n",
    "        # mas não iremos basear a escolha dos parâmetros no conjunto de teste.\n",
    "        studies = []\n",
    "        fold_params = []\n",
    "        fold_test_metrics = []\n",
    "\n",
    "        # Loop do k-fold externo\n",
    "        for train_idx, test_idx in outer_cv.split(X, y):\n",
    "            X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "            y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "            # Executa o estudo do Optuna para o fold atual\n",
    "            study = run_optuna_study_for_fold(model_name, model, X_train, y_train, inner_folds, metric, n_trials=20)\n",
    "            studies.append(study)  # Append the study to the list\n",
    "\n",
    "            # Obtém os melhores parâmetros do estudo\n",
    "            fold_best_params = {f'Modelo__{key}': value for key, value in study.best_params.items()}\n",
    "\n",
    "            # Retira os pares de strings solver_penalty, caso presentes\n",
    "            if 'Modelo__solver_penalty' in fold_best_params:\n",
    "                del fold_best_params['Modelo__solver_penalty']\n",
    "\n",
    "            # Avalia no conjunto de teste com os melhores parâmetros\n",
    "            metrics = evaluate_best_params(model, fold_best_params, X_train, y_train, X_test, y_test)\n",
    "            metrics['Model'] = model_name\n",
    "            results.append(metrics)\n",
    "\n",
    "            fold_params.append(fold_best_params)\n",
    "            fold_test_metrics.append(metrics['F1 Score'])\n",
    "\n",
    "        # Aqui, best_study_per_model, best_params_per_model, e best_f1_per_model não usam o conjunto de teste para escolha,\n",
    "        # mas iremos apenas reportar os melhores resultados encontrados internamente.\n",
    "        # Como cada fold externo produz seu próprio conjunto de parâmetros, \n",
    "        # não há um único \"melhor\" conjunto global, pois cada outer fold é independente.\n",
    "        # Podemos, por exemplo, escolher o primeiro estudo ou o estudo com melhor média interna.\n",
    "        best_study_per_model[model_name] = studies[0]\n",
    "        best_params_per_model[model_name] = fold_params[0]  # por exemplo, o primeiro fold\n",
    "        best_f1_per_model[model_name] = sum(fold_test_metrics) / len(fold_test_metrics)\n",
    "\n",
    "        all_fold_params_per_model[model_name] = fold_params\n",
    "\n",
    "    # Retorna os resultados consolidados, o melhor estudo, os melhores parâmetros e a melhor pontuação F1 por modelo\n",
    "    return pd.DataFrame(results), best_study_per_model, best_params_per_model, best_f1_per_model, all_fold_params_per_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna_algo_dict = {\n",
    "    'Logistic Regression': algo_dict['Logistic Regression'],\n",
    "    'AdaBoost': algo_dict['AdaBoost'],\n",
    "    'SVM': algo_dict['SVM']\n",
    "}\n",
    "\n",
    "# Executa a validação cruzada aninhada com otimização de hiperparâmetros via Optuna\n",
    "results_df, best_study_per_model, best_params_per_model, best_f1_per_model, all_fold_params_per_model = nested_cv_with_optuna(X, y, optuna_algo_dict, outer_folds=5, inner_folds=3, metric='f1')\n",
    "display(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name in best_params_per_model:\n",
    "    print(f\"Best parameters for {model_name}: {best_params_per_model[model_name]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, fold_params_list in all_fold_params_per_model.items():\n",
    "    for fold_idx, fold_params in enumerate(fold_params_list, start=1):\n",
    "        print(f\"Parameters for {model_name}, Fold {fold_idx}: {fold_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_study = best_study_per_model[model_name]\n",
    "print(best_study.best_params)  # Dictionary of best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exibe os melhores resultados e parâmetros para cada modelo\n",
    "for model_name in best_study_per_model:\n",
    "    print(f\"Best F1 for {model_name}: {best_f1_per_model[model_name]}\")\n",
    "    print(f\"Best Parameters for {model_name}: {best_params_per_model[model_name]}\")\n",
    "\n",
    "    print(f\"Plots for model: {model_name}\")\n",
    "\n",
    "    study = best_study_per_model[model_name]\n",
    "    fig_opt_history = plot_optimization_history(study)\n",
    "    fig_opt_history.show()\n",
    "\n",
    "    fig_slice = plot_slice(study)\n",
    "    fig_slice.show()\n",
    "\n",
    "    fig_parallel = plot_parallel_coordinate(study)\n",
    "    fig_parallel.show()\n",
    "\n",
    "    fig_contour = plot_contour(study)\n",
    "    fig_contour.show()\n",
    "\n",
    "    fig_param_importances = plot_param_importances(study)\n",
    "    fig_param_importances.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Boxplot de F1\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(data=results_df, x='Model', y='F1 Score', hue='Model', palette='Set3')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "# Boxplot da precisão\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(data=results_df, x='Model', y='Precision', hue='Model', palette='Set3')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "# Boxplot do recall\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(data=results_df, x='Model', y='Recall', hue='Model', palette='Set3')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "# Boxplot da ROC AUC\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(data=results_df, x='Model', y='ROC AUC', hue='Model', palette='Set3')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
